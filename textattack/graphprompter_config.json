{
    "model_name": "graph_llm",
    "project": "graph_prompt_tuning",
    "seed": 0,
    "dataset": "cora_sup",
    "lr": 1e-5,
    "wd": 0.05,
    "patience": 30,
    "min_lr": 5e-6,
    "resume": "",
    "batch_size": 32,
    "grad_steps": 2,
    "num_epochs": 5,
    "warmup_epochs": 1,
    "eval_batch_size": 1,
    "llm_model_name": "7b",
    "llm_model_path": "/scratch/qz2086/hard-label-attack/graphprompter/Llama-2-7b-hf",
    "llm_frozen": "True",
    "llm_num_virtual_tokens": 10,
    "output_dir": "graphprompter/output",
    "max_txt_len": 512,
    "max_new_tokens": 32,
    "adapter_len": 10,
    "adapter_layer": 30,
    "log_dir": "logs/",
    "device": "cuda:0",
    "world_size": 1,
    "local_rank": -1,
    "gpu": "0",
    "rank": -1,
    "dist_on_itp": false,
    "dist_url": "env://",
    "num_workers": 8,
    "gnn_model_name": "gat",
    "gnn_num_layers": 4,
    "gnn_in_dim": 1024,
    "gnn_hidden_dim": 1024,
    "gnn_out_dim": 1024,
    "gnn_num_heads": 4,
    "gnn_dropout": 0.0
    }
  